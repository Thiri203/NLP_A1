{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bceb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ====== SETUP (must run before Dataset / models) ======\n",
    "import re, math, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader  # <-- fixes Dataset error\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # <-- fixes device error\n",
    "print(\"device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download reuters dataset\n",
    "nltk.download('reuters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b2ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num docs: 10788\n",
      "first doc:\n",
      " ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Japan has raised fears among many of Asia's exporting\n",
      "  nations that the row could inflict far-reaching economic\n",
      "  damage, businessmen and officials said.\n",
      "      They told Reuter correspondents in Asian capitals a U.S.\n",
      "  Move against Japan might boost protectionist sentiment in the\n",
      "  U.S. And lead to curbs on American imports of their products.\n",
      "      But some exporters said that while the conflict would hurt\n",
      "  them in the long-run, in the short-term Tokyo's loss might be\n",
      "  their gain.\n",
      "      The U.S. Has said it will impose 300 mln dlrs of tariffs on\n",
      "  imports of Japanese electronics goods on April 17, in\n",
      "  retaliation for Japan's alleged failure to stick to a pact not\n",
      "  to sell semiconductors on world markets at below cost.\n",
      "      Unofficial Japanese estimates put the impact of the tariffs\n",
      "  at 10 billion dlrs and spokesmen for major electronics firms\n",
      "  said they would virtually halt exports of products hit by the\n",
      "  new taxes.\n",
      "      \"We wouldn't be able to do business,\" said a spokesman for\n",
      "  leading Japanese electronics firm Matsushita Electric\n",
      "  Industrial Co Ltd &lt;MC.T>.\n",
      "      \"If the tariffs remain in place for any length of time\n",
      "  beyond a few months it will mean the complete erosion of\n",
      "  exports (of goods subject to tariffs) to the U.S.,\" said Tom\n",
      "  Murtha, a stock analyst at the Tokyo office of broker &lt;James\n",
      "  Capel and Co>.\n",
      "      In Taiwan, businessmen and officials are also worried.\n",
      "      \"We are aware of the seriousness of the U.S. Threat against\n",
      "  Japan because it serves as a warning to us,\" said a senior\n",
      "  Taiwanese trade official who asked not to be named.\n",
      "      Taiwan had a trade trade surplus of 15.6 billion dlrs last\n",
      "  year, 95 pct of it with the U.S.\n",
      "      The surplus helped swell Taiwan's foreign exchange reserves\n",
      "  to 53 billion dlrs, among the world's largest.\n",
      "      \"We must quickly open our markets, remove trade barriers and\n",
      "  cut import tariffs to allow imports of U.S. Products, if we\n",
      "  want to defuse problems from possible U.S. Retaliation,\" said\n",
      "  Paul Sheen, chairman of textile exporters &lt;Taiwan Safe Group>.\n",
      "      A senior official of South Korea's trade promotion\n",
      "  association said the trade dispute between the U.S. And Japan\n",
      "  might also lead to pressure on South Korea, whose chief exports\n",
      "  are similar to those of Japan.\n",
      "      Last year South Korea had a trade surplus of 7.1 billion\n",
      "  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.\n",
      "      In Malaysia, trade officers and businessmen said tough\n",
      "  curbs against Japan might allow hard-hit producers of\n",
      "  semiconductors in third countries to expand their sales to the\n",
      "  U.S.\n",
      "      In Hong Kong, where newspapers have alleged Japan has been\n",
      "  selling below-cost semiconductors, some electronics\n",
      "  manufacturers share that view. But other businessmen said such\n",
      "  a short-term commercial advantage would be outweighed by\n",
      "  further U.S. Pressure to block imports.\n",
      "      \"That is a very short-term view,\" said Lawrence Mills,\n",
      "  director-general of the Federation of Hong Kong Industry.\n",
      "      \"If the whole purpose is to prevent imports, one day it will\n",
      "  be extended to other sources. Much more serious for Hong Kong\n",
      "  is the disadvantage of action restraining trade,\" he said.\n",
      "      The U.S. Last year was Hong Kong's biggest export market,\n",
      "  accounting for over 30 pct of domestically produced exports.\n",
      "      The Australian government is awaiting the outcome of trade\n",
      "  talks between the U.S. And Japan with interest and concern,\n",
      "  Industry Minister John Button said in Canberra last Friday.\n",
      "      \"This kind of deterioration in trade relations between two\n",
      "  countries which are major trading partners of ours is a very\n",
      "  serious matter,\" Button said.\n",
      "      He said Australia's concerns centred on coal and beef,\n",
      "  Australia's two largest exports to Japan and also significant\n",
      "  U.S. Exports to that country.\n",
      "      Meanwhile U.S.-Japanese diplomatic manoeuvres to solve the\n",
      "  trade stand-off continue.\n",
      "      Japan's ruling Liberal Democratic Party yesterday outlined\n",
      "  a package of economic measures to boost the Japanese economy.\n",
      "      The measures proposed include a large supplementary budget\n",
      "  and record public works spending in the first half of the\n",
      "  financial year.\n",
      "      They also call for stepped-up spending as an emergency\n",
      "  measure to stimulate the economy despite Prime Minister\n",
      "  Yasuhiro Nakasone's avowed fiscal reform program.\n",
      "      Deputy U.S. Trade Representative Michael Smith and Makoto\n",
      "  Kuroda, Japan's deputy minister of International Trade and\n",
      "  Industry (MITI), are due to meet in Washington this week in an\n",
      "  effort to end the dispute.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "doc_ids = reuters.fileids()\n",
    "docs_raw = [reuters.raw(doc_id) for doc_id in doc_ids]\n",
    "\n",
    "print(\"num docs:\", len(docs_raw))\n",
    "print(\"first doc:\\n\", docs_raw[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fedd05d",
   "metadata": {},
   "source": [
    "Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def preprocess(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  #remove except letters\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) >= 2]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences: 53232\n",
      "sample: ['asian', 'exporters', 'fear', 'damage', 'from', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of']\n"
     ]
    }
   ],
   "source": [
    "all_sentences = []\n",
    "\n",
    "for raw in docs_raw:\n",
    "    for s in sent_tokenize(raw):\n",
    "        toks = preprocess(s)\n",
    "        if len(toks) >= 3:  # keep only sentences with 3 or more tokens\n",
    "            all_sentences.append(toks)\n",
    "\n",
    "print(\"num sentences:\", len(all_sentences))\n",
    "print(\"sample:\", all_sentences[0][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7338e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg tokens per sentence: 23.96\n",
      "0 ['asian', 'exporters', 'fear', 'damage', 'from', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said']\n",
      "1 ['they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products']\n",
      "2 ['but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short', 'term', 'tokyo', 'loss', 'might', 'be', 'their', 'gain']\n"
     ]
    }
   ],
   "source": [
    "avg_len = sum(len(s) for s in all_sentences) / len(all_sentences)\n",
    "print(\"avg tokens per sentence:\", round(avg_len, 2))\n",
    "\n",
    "for i in range(3):\n",
    "    print(i, all_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0219c85",
   "metadata": {},
   "source": [
    "Step 2: Vocabulary & numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c662e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'has',\n",
       " 'said',\n",
       " 'it',\n",
       " 'will',\n",
       " 'impose',\n",
       " 'mln',\n",
       " 'dlrs',\n",
       " 'of',\n",
       " 'tariffs',\n",
       " 'on',\n",
       " 'imports',\n",
       " 'of',\n",
       " 'japanese',\n",
       " 'electronics',\n",
       " 'goods',\n",
       " 'on',\n",
       " 'april',\n",
       " 'in',\n",
       " 'retaliation',\n",
       " 'for',\n",
       " 'japan',\n",
       " 'alleged',\n",
       " 'failure',\n",
       " 'to',\n",
       " 'stick',\n",
       " 'to',\n",
       " 'pact',\n",
       " 'not',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'semiconductors',\n",
       " 'on',\n",
       " 'world',\n",
       " 'markets',\n",
       " 'at',\n",
       " 'below',\n",
       " 'cost']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences[3]  # list of tokenized sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c3c13",
   "metadata": {},
   "source": [
    "Step 2.1: Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004afd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 29149\n",
      "[('the', 69256), ('of', 36778), ('to', 36400), ('in', 29247), ('and', 25648), ('said', 25362), ('mln', 18623), ('vs', 14340), ('for', 13781), ('dlrs', 12407)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# flatten all sentences into one list\n",
    "all_tokens = []\n",
    "for sent in all_sentences:\n",
    "    all_tokens.extend(sent)\n",
    "\n",
    "# count word frequencies\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "print(\"Total unique words:\", len(word_counts))\n",
    "print(word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e4de0",
   "metadata": {},
   "source": [
    "Step 2.2: Limit vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab size: 10398\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 20000 \n",
    "MIN_COUNT = 5  # ignore very rare words\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "vocab = [\n",
    "    word for word, count in word_counts.items()\n",
    "    if count >= MIN_COUNT\n",
    "]\n",
    "\n",
    "vocab = vocab[:MAX_VOCAB_SIZE]\n",
    "\n",
    "print(\"Final vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58d39d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocab size (with UNK): 10399\n",
      "First 10 vocab items: ['<UNK>', 'asian', 'exporters', 'fear', 'damage', 'from', 'japan', 'mounting', 'trade', 'friction']\n"
     ]
    }
   ],
   "source": [
    "vocab = [UNK_TOKEN] + vocab\n",
    "print(\"Final vocab size (with UNK):\", len(vocab))\n",
    "print(\"First 10 vocab items:\", vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6d4eb",
   "metadata": {},
   "source": [
    "Step 2.3: Create word â†” id mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ae9cc",
   "metadata": {},
   "source": [
    "Map each word to a unique integer index so it can be used by embedding layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b8c9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK token id: 0\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "print(\"UNK token id:\", word2idx[UNK_TOKEN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0c086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10399\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a3dcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word index: 10398\n"
     ]
    }
   ],
   "source": [
    "print(\"Max word index:\", max(word2idx.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0dca37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"oil\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5caad982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"<UNK>\"] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3bf5a2",
   "metadata": {},
   "source": [
    "Step 2.4: Convert sentences to word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6709b29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4301,\n",
       " 3847,\n",
       " 706,\n",
       " 159,\n",
       " 945,\n",
       " 1162,\n",
       " 2205,\n",
       " 1163,\n",
       " 2205,\n",
       " 1162,\n",
       " 2189,\n",
       " 2205,\n",
       " 1163,\n",
       " 2205,\n",
       " 945,\n",
       " 1163,\n",
       " 2251,\n",
       " 69,\n",
       " 1163,\n",
       " 69]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word2idx[w] for w in sent if w in word2idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de65c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids = [\n",
    "    word2idx.get(w, word2idx[\"<UNK>\"])\n",
    "    for w in sent\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f97f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_ids = []\n",
    "\n",
    "for sent in all_sentences:\n",
    "    ids = [\n",
    "        word2idx.get(w, word2idx[UNK_TOKEN])\n",
    "        for w in sent\n",
    "    ]\n",
    "    if len(ids) >= 2:\n",
    "        sentences_ids.append(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ff399ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences (ID form): 53232\n",
      "Example sentence (words): ['asian', 'exporters', 'fear', 'damage', 'from', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has']\n",
      "Example sentence (ids): [1, 2, 3, 4, 5, 6, 0, 7, 8, 9, 10, 11, 12, 6, 13]\n",
      "Test unknown word -> id: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences (ID form):\", len(sentences_ids))\n",
    "print(\"Example sentence (words):\", all_sentences[0][:15])\n",
    "print(\"Example sentence (ids):\", sentences_ids[0][:15])\n",
    "\n",
    "# test unknown word explicitly\n",
    "print(\"Test unknown word -> id:\",\n",
    "      word2idx.get(\"thisworddoesnotexist\", word2idx[UNK_TOKEN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c19d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary files saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "with open(\"artifacts/word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "\n",
    "with open(\"artifacts/idx2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(idx2word, f)\n",
    "\n",
    "\n",
    "print(\"Vocabulary files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd655a",
   "metadata": {},
   "source": [
    "Skip-gram pair generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "133c7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(sentences_ids, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate (center, context) word pairs for Skip-gram training.\n",
    "\n",
    "    Args:\n",
    "        sentences_ids (list of list of int): tokenized sentences as word indices\n",
    "        window_size (int): context window size on each side\n",
    "\n",
    "    Returns:\n",
    "        list of (int, int): (center_word, context_word) pairs\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "\n",
    "    for sentence in sentences_ids:\n",
    "        sentence_length = len(sentence)\n",
    "\n",
    "        for center_pos, center_word in enumerate(sentence):\n",
    "            start = max(0, center_pos - window_size)\n",
    "            end = min(sentence_length, center_pos + window_size + 1)\n",
    "\n",
    "            for context_pos in range(start, end):\n",
    "                if context_pos != center_pos:\n",
    "                    context_word = sentence[context_pos]\n",
    "                    pairs.append((center_word, context_word))\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3bfae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs: 4782684\n",
      "First 5 pairs: [(1, 2), (1, 3), (2, 1), (2, 3), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "pairs = generate_skipgram_pairs(sentences_ids, window_size=2)\n",
    "\n",
    "print(\"Number of training pairs:\", len(pairs))\n",
    "print(\"First 5 pairs:\", pairs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c2f20",
   "metadata": {},
   "source": [
    "Skip-gram Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c44f1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Skip-gram (center, context) word pairs\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center_word, context_word = self.pairs[idx]\n",
    "        return (\n",
    "            torch.tensor(center_word, dtype=torch.long),\n",
    "            torch.tensor(context_word, dtype=torch.long)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9082763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 37365\n"
     ]
    }
   ],
   "source": [
    "dataset = SkipGramDataset(pairs)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Number of batches:\", len(dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4bcf4b",
   "metadata": {},
   "source": [
    "Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6161089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram model using two embedding matrices:\n",
    "    - input embeddings for center words\n",
    "    - output embeddings for context words\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center_words):\n",
    "        \"\"\"\n",
    "        center_words: Tensor of shape (batch_size,)\n",
    "        returns: center word embeddings of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.input_embeddings(center_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd490d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Skip-gram model\n",
    "embedding_dim = 100 \n",
    "\n",
    "skipgram_model = SkipGram(vocab_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(skipgram_model.parameters(), lr=0.001)\n",
    "\n",
    "# Track losses\n",
    "skipgram_losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d16b",
   "metadata": {},
   "source": [
    "Skip-gram loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f53b043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_loss(center_vectors, context_words, model):\n",
    "    \"\"\"\n",
    "    center_vectors: (batch_size, embedding_dim)\n",
    "    context_words: (batch_size,)\n",
    "    \"\"\"\n",
    "    context_vectors = model.output_embeddings(context_words)\n",
    "\n",
    "    # dot product between center and context vectors\n",
    "    scores = torch.sum(center_vectors * context_vectors, dim=1)\n",
    "\n",
    "    # negative log likelihood\n",
    "    loss = -torch.mean(torch.log(torch.sigmoid(scores)))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247cacf",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a181fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_earlystop(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    max_epochs=10,\n",
    "    patience=2,\n",
    "    min_delta=1e-4\n",
    "):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for center_words, context_words in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            center_vectors = model(center_words)\n",
    "            loss = skipgram_loss(center_vectors, context_words, model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        print(f\"[SG] Epoch {epoch+1}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        if best_loss - avg_loss < min_delta:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered (Skip-gram)\")\n",
    "                break\n",
    "        else:\n",
    "            best_loss = avg_loss\n",
    "            no_improve = 0\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ef985",
   "metadata": {},
   "source": [
    "Train Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "354c3000",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      3\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m skipgram_losses = \u001b[43mtrain_skipgram_earlystop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipgram_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m skipgram_time = time.time() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_skipgram_earlystop\u001b[39m\u001b[34m(model, dataloader, optimizer, max_epochs, patience, min_delta)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[32m     16\u001b[39m     total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcenter_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcenter_vectors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLP\\A1_AIT\\A1_st126018\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:728\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[32m    731\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NLP\\A1_AIT\\A1_st126018\\.venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:789\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = torch.ops.profiler._record_function_enter_new(\n\u001b[32m    785\u001b[39m         \u001b[38;5;28mself\u001b[39m.name, \u001b[38;5;28mself\u001b[39m.args\n\u001b[32m    786\u001b[39m     )\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_callbacks_on_exit:\n\u001b[32m    791\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "skipgram_losses = train_skipgram_earlystop(\n",
    "    skipgram_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    max_epochs=10\n",
    ")\n",
    "\n",
    "skipgram_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "with open(\"artifacts/skipgram_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model_state_dict\": skipgram_model.state_dict(),\n",
    "        \"word2idx\": word2idx,\n",
    "        \"idx2word\": idx2word,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embedding_dim\": embedding_dim\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45d3a8",
   "metadata": {},
   "source": [
    "Negative Sampling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegDataset(Dataset):\n",
    "    def __init__(self, pairs, vocab_size, num_negatives=5):\n",
    "        self.pairs = pairs\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center_word, pos_context = self.pairs[idx]\n",
    "\n",
    "        neg_contexts = torch.randint(\n",
    "            low=0,\n",
    "            high=self.vocab_size,\n",
    "            size=(self.num_negatives,),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            torch.tensor(center_word, dtype=torch.long),\n",
    "            torch.tensor(pos_context, dtype=torch.long),\n",
    "            neg_contexts\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af296f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dataset = SkipGramNegDataset(\n",
    "    pairs,\n",
    "    vocab_size=vocab_size,\n",
    "    num_negatives=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f601fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dataloader = DataLoader(\n",
    "    neg_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828d1f5",
   "metadata": {},
   "source": [
    "Negative Sampling Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae34f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sampling_loss(center_vectors, pos_context, neg_contexts, model):\n",
    "    pos_vectors = model.output_embeddings(pos_context)\n",
    "    neg_vectors = model.output_embeddings(neg_contexts)\n",
    "\n",
    "    pos_score = torch.sum(center_vectors * pos_vectors, dim=1)\n",
    "    pos_loss = torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "    neg_score = torch.bmm(\n",
    "        neg_vectors, center_vectors.unsqueeze(2)\n",
    "    ).squeeze()\n",
    "    neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score)), dim=1)\n",
    "\n",
    "    return -(pos_loss + neg_loss).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4ec0c",
   "metadata": {},
   "source": [
    "Training Loop (NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_neg_earlystop(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    max_epochs=10,\n",
    "    patience=2,\n",
    "    min_delta=1e-3\n",
    "):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for center_words, pos_context, neg_contexts in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            center_vectors = model(center_words)\n",
    "            loss = neg_sampling_loss(\n",
    "                center_vectors, pos_context, neg_contexts, model\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"[NEG] Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if best_loss - avg_loss < min_delta:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered (NEG)\")\n",
    "                break\n",
    "        else:\n",
    "            best_loss = avg_loss\n",
    "            no_improve = 0\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2733adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_neg_model = SkipGram(vocab_size, embedding_dim)\n",
    "optimizer_neg = torch.optim.Adam(skipgram_neg_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f5b1f",
   "metadata": {},
   "source": [
    "Train NEG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef484101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEG] Epoch 1, Loss: 5.7840\n",
      "[NEG] Epoch 2, Loss: 1.5392\n",
      "[NEG] Epoch 3, Loss: 1.2280\n",
      "[NEG] Epoch 4, Loss: 1.1168\n",
      "[NEG] Epoch 5, Loss: 1.0584\n",
      "[NEG] Epoch 6, Loss: 1.0215\n",
      "[NEG] Epoch 7, Loss: 0.9972\n",
      "[NEG] Epoch 8, Loss: 0.9797\n",
      "[NEG] Epoch 9, Loss: 0.9670\n",
      "[NEG] Epoch 10, Loss: 0.9570\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "neg_losses = train_skipgram_neg_earlystop(\n",
    "    skipgram_neg_model,\n",
    "    neg_dataloader,\n",
    "    optimizer_neg,\n",
    "    max_epochs=10\n",
    ")\n",
    "neg_time = time.time() - start_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eef670",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/skipgram_neg_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model_state_dict\": skipgram_neg_model.state_dict(),\n",
    "        \"word2idx\": word2idx,\n",
    "        \"idx2word\": idx2word,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embedding_dim\": embedding_dim\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca3707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram training time: 811.89 s\n",
      "NEG training time: 1532.75 s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Skip-gram training time: {skipgram_time:.2f} s\")\n",
    "print(f\"NEG training time: {neg_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b526667",
   "metadata": {},
   "source": [
    "GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1ccda",
   "metadata": {},
   "source": [
    "Build co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc90c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence pairs: 1030764\n"
     ]
    }
   ],
   "source": [
    "def build_cooccurrence(sentences_ids, window_size=2):\n",
    "    cooc = defaultdict(float)\n",
    "\n",
    "    for sent in sentences_ids:\n",
    "        for i, wi in enumerate(sent):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sent), i + window_size + 1)\n",
    "\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    wj = sent[j]\n",
    "                    cooc[(wi, wj)] += 1.0 / abs(i - j)\n",
    "\n",
    "    return cooc\n",
    "\n",
    "\n",
    "cooc_matrix = build_cooccurrence(sentences_ids, window_size=2)\n",
    "print(\"Co-occurrence pairs:\", len(cooc_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e570e0a",
   "metadata": {},
   "source": [
    "initialize GloVe parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "X_MAX = 100\n",
    "ALPHA = 0.75\n",
    "LR = 0.001 \n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "W = rng.normal(0, 0.01, (vocab_size, embedding_dim))\n",
    "W_tilde = rng.normal(0, 0.01, (vocab_size, embedding_dim))\n",
    "b = np.zeros(vocab_size)\n",
    "b_tilde = np.zeros(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aab6b7",
   "metadata": {},
   "source": [
    "GloVe weighting function and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_weight(x, x_max=X_MAX, alpha=ALPHA):\n",
    "    if x < x_max:\n",
    "        return (x / x_max) ** alpha\n",
    "    return 1.0\n",
    "\n",
    "def train_glove(\n",
    "    cooc_matrix,\n",
    "    W,\n",
    "    W_tilde,\n",
    "    b,\n",
    "    b_tilde,\n",
    "    epochs=10,\n",
    "    lr=LR,\n",
    "    grad_clip=5.0\n",
    "):\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for (i, j), x in cooc_matrix.items():\n",
    "            if x <= 0:\n",
    "                continue\n",
    "\n",
    "            w_ij = glove_weight(x)\n",
    "\n",
    "            dot = np.dot(W[i], W_tilde[j])\n",
    "            dot = np.clip(dot, -10, 10)      # ðŸ”¥ prevents overflow\n",
    "\n",
    "            diff = dot + b[i] + b_tilde[j] - np.log(x)\n",
    "            loss = w_ij * diff * diff\n",
    "            total_loss += loss\n",
    "\n",
    "            grad = w_ij * diff\n",
    "            grad = np.clip(grad, -grad_clip, grad_clip)\n",
    "\n",
    "            W_i_old = W[i].copy()\n",
    "\n",
    "            W[i] -= lr * grad * W_tilde[j]\n",
    "            W_tilde[j] -= lr * grad * W_i_old\n",
    "            b[i] -= lr * grad\n",
    "            b_tilde[j] -= lr * grad\n",
    "\n",
    "        losses.append(total_loss)\n",
    "        print(f\"[GloVe] Epoch {epoch+1}, Loss: {total_loss:.2e}\")\n",
    "\n",
    "        if not np.isfinite(total_loss):\n",
    "            print(\"NaN detected â€” stopping\")\n",
    "            break\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GloVe] Epoch 1, Loss: 2.33e+05\n",
      "[GloVe] Epoch 2, Loss: 1.65e+05\n",
      "[GloVe] Epoch 3, Loss: 1.40e+05\n",
      "[GloVe] Epoch 4, Loss: 1.27e+05\n",
      "[GloVe] Epoch 5, Loss: 1.19e+05\n",
      "[GloVe] Epoch 6, Loss: 1.13e+05\n",
      "[GloVe] Epoch 7, Loss: 1.09e+05\n",
      "[GloVe] Epoch 8, Loss: 1.06e+05\n",
      "[GloVe] Epoch 9, Loss: 1.04e+05\n",
      "[GloVe] Epoch 10, Loss: 1.02e+05\n",
      "Training time: 70.42591309547424\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "glove_losses = train_glove(\n",
    "    cooc_matrix,\n",
    "    W,\n",
    "    W_tilde,\n",
    "    b,\n",
    "    b_tilde,\n",
    "    epochs=10,\n",
    "    lr=0.001\n",
    ")\n",
    "print(\"Training time:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = W + W_tilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35567606",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "glove_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artifacts/glove_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": glove_embeddings,\n",
    "        \"word2idx\": word2idx,\n",
    "        \"idx2word\": idx2word\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c62463",
   "metadata": {},
   "source": [
    "Task 2 Evaluation (ALL 3 MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Extract trained embeddings =====\n",
    "\n",
    "# Skip-gram\n",
    "sg_embeddings = skipgram_model.input_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# Negative Sampling\n",
    "neg_embeddings = skipgram_neg_model.input_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# GloVe\n",
    "glove_embeddings = W + W_tilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / (norms + 1e-10)\n",
    "\n",
    "sg_embeddings_norm = normalize_embeddings(sg_embeddings)\n",
    "neg_embeddings_norm = normalize_embeddings(neg_embeddings)\n",
    "glove_embeddings_norm = normalize_embeddings(glove_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogy_dataset(path):\n",
    "    analogies = []\n",
    "    current_category = None\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Category line\n",
    "            if line.startswith(\":\"):\n",
    "                current_category = line[2:]\n",
    "                continue\n",
    "\n",
    "            parts = line.lower().split()\n",
    "\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "\n",
    "            a, b, c, d = parts\n",
    "            analogies.append((current_category, a, b, c, d))\n",
    "\n",
    "    return analogies\n",
    "\n",
    "analogy_data = load_analogy_dataset(\"word-test.v1.txt\")\n",
    "\n",
    "\n",
    "def analogy_predict(a, b, c, embeddings, word2idx, idx2word):\n",
    "    for w in (a, b, c):\n",
    "        if w not in word2idx:\n",
    "            return None\n",
    "\n",
    "    va = embeddings[word2idx[a]]\n",
    "    vb = embeddings[word2idx[b]]\n",
    "    vc = embeddings[word2idx[c]]\n",
    "\n",
    "    target = vb - va + vc\n",
    "    target /= np.linalg.norm(target) + 1e-10\n",
    "\n",
    "    sims = embeddings @ target\n",
    "    for w in (a, b, c):\n",
    "        sims[word2idx[w]] = -np.inf\n",
    "\n",
    "    return idx2word[np.argmax(sims)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogies(analogies, embeddings, word2idx, idx2word):\n",
    "    semantic_correct = semantic_total = 0\n",
    "    syntactic_correct = syntactic_total = 0\n",
    "\n",
    "    for category, a, b, c, d in analogies:\n",
    "        pred = analogy_predict(a, b, c, embeddings, word2idx, idx2word)\n",
    "        if pred is None:\n",
    "            continue\n",
    "\n",
    "        if category.startswith(\"gram\"):\n",
    "            syntactic_total += 1\n",
    "            if pred == d:\n",
    "                syntactic_correct += 1\n",
    "        else:\n",
    "            semantic_total += 1\n",
    "            if pred == d:\n",
    "                semantic_correct += 1\n",
    "\n",
    "    return {\n",
    "        \"semantic_acc\": semantic_correct / max(1, semantic_total),\n",
    "        \"syntactic_acc\": syntactic_correct / max(1, syntactic_total),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c7452",
   "metadata": {},
   "source": [
    "Run evaluation for ALL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram: {'semantic_acc': 0.0, 'syntactic_acc': 0.0007015902712815715}\n",
      "Skip-gram + NEG: {'semantic_acc': 0.020955574182732608, 'syntactic_acc': 0.011693171188026192}\n",
      "GloVe: {'semantic_acc': 0.0, 'syntactic_acc': 0.00023386342376052386}\n"
     ]
    }
   ],
   "source": [
    "sg_analogy = evaluate_analogies(\n",
    "    analogy_data,\n",
    "    sg_embeddings_norm,\n",
    "    word2idx,\n",
    "    idx2word\n",
    ")\n",
    "\n",
    "neg_analogy = evaluate_analogies(\n",
    "    analogy_data,\n",
    "    neg_embeddings_norm,\n",
    "    word2idx,\n",
    "    idx2word\n",
    ")\n",
    "\n",
    "glove_analogy = evaluate_analogies(\n",
    "    analogy_data,\n",
    "    glove_embeddings_norm,\n",
    "    word2idx,\n",
    "    idx2word\n",
    ")\n",
    "\n",
    "print(\"Skip-gram:\", sg_analogy)\n",
    "print(\"Skip-gram + NEG:\", neg_analogy)\n",
    "print(\"GloVe:\", glove_analogy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88d360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (Mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission</td>\n",
       "      <td>ticket</td>\n",
       "      <td>5.5360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>4.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>6.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>announcement</td>\n",
       "      <td>effort</td>\n",
       "      <td>2.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announcement</td>\n",
       "      <td>news</td>\n",
       "      <td>7.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 1     Word 2  Human (Mean)\n",
       "0     admission     ticket        5.5360\n",
       "1       alcohol  chemistry        4.1250\n",
       "2      aluminum      metal        6.6250\n",
       "3  announcement     effort        2.0625\n",
       "4  announcement       news        7.1875"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wordsim_df = pd.read_csv(\"wordsim353.csv\")\n",
    "wordsim_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a10c0",
   "metadata": {},
   "source": [
    "Utility functions (dot product + evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51083a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def get_dot_product(w1, w2, embeddings, word2idx):\n",
    "    if w1 not in word2idx or w2 not in word2idx:\n",
    "        return None\n",
    "    return np.dot(embeddings[word2idx[w1]], embeddings[word2idx[w2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda4f65",
   "metadata": {},
   "source": [
    "Compute Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63450b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wordsim(embeddings, word2idx, wordsim_df):\n",
    "    model_scores = []\n",
    "    human_scores = []\n",
    "\n",
    "    for _, row in wordsim_df.iterrows():\n",
    "        w1 = row[\"Word 1\"].lower()\n",
    "        w2 = row[\"Word 2\"].lower()\n",
    "        human_score = row[\"Human (Mean)\"]\n",
    "\n",
    "        score = get_dot_product(w1, w2, embeddings, word2idx)\n",
    "        if score is not None:\n",
    "            model_scores.append(score)\n",
    "            human_scores.append(human_score)\n",
    "\n",
    "    corr, _ = spearmanr(model_scores, human_scores)\n",
    "    return corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa7d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Spearman Correlation:\n",
      "Skip-gram: -0.07303293005723953\n",
      "Skip-gram + NEG: 0.26210855852372505\n",
      "GloVe: 0.13667895006713254\n"
     ]
    }
   ],
   "source": [
    "sg_wordsim = evaluate_wordsim(sg_embeddings, word2idx, wordsim_df)\n",
    "neg_wordsim = evaluate_wordsim(neg_embeddings, word2idx, wordsim_df)\n",
    "glove_wordsim = evaluate_wordsim(glove_embeddings, word2idx, wordsim_df)\n",
    "\n",
    "print(\"WordSim Spearman Correlation:\")\n",
    "print(\"Skip-gram:\", sg_wordsim)\n",
    "print(\"Skip-gram + NEG:\", neg_wordsim)\n",
    "print(\"GloVe:\", glove_wordsim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13462cf",
   "metadata": {},
   "source": [
    "| Model           | Epochs          | Training Time (s) | Final Loss     | WordSim (Ï) | Semantic Acc | Syntactic Acc |\n",
    "| --------------- | --------------- | ----------------- | -------------- | ----------- | ------------ | ------------- |\n",
    "| Skip-gram       | â‰¤7 (early stop) | **811.89**        | ~0.0000        | **âˆ’0.073**  | 0.0000       | 0.0007        |\n",
    "| Skip-gram + NEG | 10              | **1532.75**       | **0.9570**     | **0.262**   | 0.0210       | 0.0117        |\n",
    "| GloVe           | 10              | **70.43**         | **1.02 Ã— 10âµ** | **0.137**   | 0.0000       | 0.0002        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c95a20",
   "metadata": {},
   "source": [
    "SG < NEG < GloVe in time\n",
    "\n",
    "Skip-gram with negative sampling requires substantially more training time than vanilla Skip-gram due to additional negative samples processed for each training pair. GloVe is significantly faster in our implementation because it operates on a sparse co-occurrence matrix rather than iterating over all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5fb228",
   "metadata": {},
   "source": [
    "Overall analysis \n",
    "\n",
    "Skip-gram trains fast and the loss becomes very small, but it does not learn good word meaning. This is shown by the negative WordSim correlation and very low accuracy. Without negative sampling, the model cannot separate correct and incorrect contexts well.\n",
    "\n",
    "Skip-gram with negative sampling takes longer time to train, but it gives the best results. It has higher semantic and syntactic accuracy and the highest correlation with human similarity scores. This shows that negative sampling helps the model learn better word representations.\n",
    "\n",
    "GloVe trains much faster than Skip-gram models and the loss decreases steadily. However, its performance on analogy tasks is still low. This may be because the model is trained for a small number of epochs and uses a simple optimization method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
